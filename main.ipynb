{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = r'C:\\Users\\Intel7\\Desktop\\MAS\\all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "      <th>OriginalDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>white hanging heart tlight holder</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>white metal lantern</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>cream cupid hearts coat hanger</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>knitted union flag hot water bottle</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>red woolly hottie white heart</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A    white hanging heart tlight holder         6   \n",
       "1    536365     71053                  white metal lantern         6   \n",
       "2    536365    84406B       cream cupid hearts coat hanger         8   \n",
       "3    536365    84029G  knitted union flag hot water bottle         6   \n",
       "4    536365    84029E        red woolly hottie white heart         6   \n",
       "\n",
       "      InvoiceDate  UnitPrice  CustomerID         Country  \\\n",
       "0  12/1/2010 8:26       2.55     17850.0  United Kingdom   \n",
       "1  12/1/2010 8:26       3.39     17850.0  United Kingdom   \n",
       "2  12/1/2010 8:26       2.75     17850.0  United Kingdom   \n",
       "3  12/1/2010 8:26       3.39     17850.0  United Kingdom   \n",
       "4  12/1/2010 8:26       3.39     17850.0  United Kingdom   \n",
       "\n",
       "                   OriginalDescription  \n",
       "0   WHITE HANGING HEART T-LIGHT HOLDER  \n",
       "1                  WHITE METAL LANTERN  \n",
       "2       CREAM CUPID HEARTS COAT HANGER  \n",
       "3  KNITTED UNION FLAG HOT WATER BOTTLE  \n",
       "4       RED WOOLLY HOTTIE WHITE HEART.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Intel7\\Desktop\\MAS\\Dataset\\data.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "df['OriginalDescription'] = df['Description']\n",
    "\n",
    "# Data Cleaning\n",
    "df.dropna(subset=['Description'], inplace=True)  # Remove rows with missing descriptions\n",
    "df['Description'] = df['Description'].str.lower()  # Convert to lowercase\n",
    "df['Description'] = df['Description'].str.replace(r'[^a-zA-Z0-9 ]', '', regex=True)  # Remove special characters\n",
    "# elimination of NaN values\n",
    "df.dropna(inplace=True)\n",
    "# elimination of duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "# elimination of cancelled orders\n",
    "df = df[~df['InvoiceNo'].str.startswith('C')]\n",
    "\n",
    "# remove duplicate descriptions \n",
    "df.drop_duplicates(subset=['Description'], inplace=True, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the product descriptions\n",
    "descriptions = df['Description'].tolist()\n",
    "encoded_descriptions = model.encode(descriptions)\n",
    "\n",
    "# Create a Faiss index\n",
    "dim = encoded_descriptions.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# Add the encoded descriptions to the Faiss index\n",
    "index.add(encoded_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2871\n",
      "BIG POLKADOT MUG\n",
      "Distance: 0.45231256\n",
      "\n",
      "1065\n",
      "GLAMOROUS  MUG\n",
      "Distance: 0.52221566\n",
      "\n",
      "1478\n",
      "POTTERING MUG\n",
      "Distance: 0.6258617\n",
      "\n",
      "925\n",
      "HOME SWEET HOME MUG\n",
      "Distance: 0.6635146\n",
      "\n",
      "1160\n",
      "LOCAL CAFE MUG\n",
      "Distance: 0.6828939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_faiss(query, model, index, k=5):\n",
    "    query = model.encode([query])[0]\n",
    "    query = np.expand_dims(query, axis=0)\n",
    "    Distances, Indices = index.search(query, k)\n",
    "    return Distances, Indices\n",
    "\n",
    "# Search for similar products\n",
    "query = \"big mug\"\n",
    "Distances, Indices = search_faiss(query, model, index)\n",
    "\n",
    "# Print the similar products\n",
    "for i in range(len(Indices[0])):\n",
    "    print(Indices[0][i])\n",
    "    print(df['OriginalDescription'].iloc[Indices[0][i]])\n",
    "    print('Distance:', Distances[0][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results:\n",
      "CRYSTAL FROG PHONE CHARM\n",
      "CRYSTAL STILETTO PHONE CHARM\n",
      "CRYSTAL SEA HORSE PHONE CHARM\n",
      "PINK CRYSTAL SKULL PHONE CHARM\n",
      "PINK CRYSTAL GUITAR PHONE CHARM\n"
     ]
    }
   ],
   "source": [
    "from agno.agent import Agent\n",
    "\n",
    "class SearchAgent(Agent):\n",
    "    def __init__(self, index, model, df):\n",
    "        super().__init__()\n",
    "        self.index = index\n",
    "        self.model = model\n",
    "        self.df = df\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        encoded_query = self.model.encode([query])\n",
    "        distances, indices = self.index.search(encoded_query, k)\n",
    "        results = [self.df['OriginalDescription'].iloc[indices[0][i]] for i in range(len(indices[0]))]\n",
    "        return results\n",
    "\n",
    "# Initialize the search agent\n",
    "search_agent = SearchAgent(index=index, model=model, df=df)\n",
    "\n",
    "# Example usage\n",
    "query = \"Phone\"\n",
    "results = search_agent.search(query)\n",
    "print(\"Search results:\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour!\n",
      "\n",
      "As a math expert, I don't have much to say about the latest French news or politics. But if you're interested in some interesting mathematical facts or connections related to France, I'd be happy to share some tidbits!\n",
      "\n",
      "For instance, did you know that the famous mathematician Blaise Pascal was born in Clermont-Ferrand, France? He's known for his work on probability theory and was a pioneer in the field of calculus.\n",
      "\n",
      "Or how about this: The French mathematician Pierre-Simon Laplace is credited with developing the nebular hypothesis, which proposes that stars and planets form from the condensation of interstellar gas. His work laid the foundation for modern astrophysics!\n",
      "\n",
      "If you're interested in more math-related fun facts or connections to France, I'd be happy to share!\n"
     ]
    }
   ],
   "source": [
    "from agno.agent import Agent, RunResponse\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    model=Ollama(id=\"llama3:latest\"),\n",
    "    markdown=False,\n",
    "    description=\"You are an math expert!\",\n",
    ")\n",
    "\n",
    "response = agent.run(\"Whats happening in France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is today's date?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Unable to decode function arguments:                                                                      \n",
       "         <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'president_of_the_united_states'</span><span style=\"font-weight: bold\">}</span>                                                               \n",
       "         Error: the JSON object must be str, bytes or bytearray, not dict                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Unable to decode function arguments:                                                                      \n",
       "         \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'president_of_the_united_states'\u001b[0m\u001b[1m}\u001b[0m                                                               \n",
       "         Error: the JSON object must be str, bytes or bytearray, not dict                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions (Request ID: Root=1-67c49f39-1aee8f564940d0577053ede2;e387f110-b291-467a-96b3-a4ced2aa83f8)\n\nFailed to deserialize the JSON body into the target type: messages[2].content: data did not match any variant of untagged enum MessageContent at line 1 column 1684\nMake sure 'text-generation' task is supported by the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is today\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms date?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSearch on wikipedia: Who is currently the president of the United States?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     28\u001b[0m response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet the current stock price and recent history for AAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\agno\\agent\\agent.py:869\u001b[0m, in \u001b[0;36mAgent.run\u001b[1;34m(self, message, stream, audio, images, videos, messages, stream_intermediate_steps, retries, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m             resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[0;32m    860\u001b[0m                 message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m    861\u001b[0m                 stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    868\u001b[0m             )\n\u001b[1;32m--> 869\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(resp)\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ModelProviderError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    871\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\agno\\agent\\agent.py:591\u001b[0m, in \u001b[0;36mAgent._run\u001b[1;34m(self, message, stream, audio, images, videos, messages, stream_intermediate_steps, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_run_response(\n\u001b[0;32m    586\u001b[0m                         content\u001b[38;5;241m=\u001b[39mmodel_response_chunk\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m    587\u001b[0m                         event\u001b[38;5;241m=\u001b[39mRunEvent\u001b[38;5;241m.\u001b[39mtool_call_completed,\n\u001b[0;32m    588\u001b[0m                     )\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;66;03m# Get the model response\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m     model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_messages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# Handle structured outputs\u001b[39;00m\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_outputs \u001b[38;5;129;01mand\u001b[39;00m model_response\u001b[38;5;241m.\u001b[39mparsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;66;03m# Update the run_response content with the structured output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\agno\\models\\base.py:163\u001b[0m, in \u001b[0;36mModel.response\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    159\u001b[0m model_response \u001b[38;5;241m=\u001b[39m ModelResponse()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Get response from model\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     assistant_message, has_tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Handle tool calls if present\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_tool_calls:\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;66;03m# Prepare function calls\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\agno\\models\\base.py:287\u001b[0m, in \u001b[0;36mModel._process_model_response\u001b[1;34m(self, messages, model_response)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[0;32m    286\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstart_timer()\n\u001b[1;32m--> 287\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstop_timer()\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Parse provider response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\agno\\models\\huggingface\\huggingface.py:237\u001b[0m, in \u001b[0;36mHuggingFace.invoke\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mSend a chat completion request to the HuggingFace Hub.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    ChatCompletionOutput: The chat completion response from the Inference Client.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InferenceTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    243\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError invoking HuggingFace model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:970\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[0;32m    943\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    962\u001b[0m }\n\u001b[0;32m    963\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    964\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    965\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    968\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    969\u001b[0m )\n\u001b[1;32m--> 970\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:327\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Intel7\\Desktop\\MAS\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions (Request ID: Root=1-67c49f39-1aee8f564940d0577053ede2;e387f110-b291-467a-96b3-a4ced2aa83f8)\n\nFailed to deserialize the JSON body into the target type: messages[2].content: data did not match any variant of untagged enum MessageContent at line 1 column 1684\nMake sure 'text-generation' task is supported by the model."
     ]
    }
   ],
   "source": [
    "from agno.agent import Agent, RunResponse\n",
    "from agno.models.huggingface import HuggingFace\n",
    "from agno.tools.googlesearch import GoogleSearchTools\n",
    "from agno.tools.wikipedia import WikipediaTools\n",
    "from agno.tools.yfinance import YFinanceTools\n",
    "import os\n",
    "\n",
    "HF_token = 'API_TOKEN'\n",
    "os.environ['HF_TOKEN'] = HF_token\n",
    "\n",
    "agent = Agent(\n",
    "    model=HuggingFace(\n",
    "        id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        max_tokens=4096,\n",
    "    ),\n",
    "    markdown=True, \n",
    "    tools = [WikipediaTools(), YFinanceTools(), GoogleSearchTools()],\n",
    "    show_tool_calls=True,\n",
    "    description=\"You are an AI assistant!\"\n",
    ")\n",
    "\n",
    "response = agent.run(\"What is today's date?\")\n",
    "print(response.content)\n",
    "\n",
    "response = agent.run(\"Search on wikipedia: Who is currently the president of the United States?\")\n",
    "print(response.content)\n",
    "\n",
    "response = agent.run(\"Get the current stock price and recent history for AAPL\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
